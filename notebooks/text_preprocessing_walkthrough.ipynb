{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836193f1-3775-4bcd-a4a2-4686480a7ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38d81864-d8b3-464a-a0e0-8f27890e7688",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70388cf4-e3ab-4f03-aabb-7bbfda9e8967",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Here you can find the contents of [The Verdict](https://)\n",
    "\n",
    "Lets read and print the total number of characters followed by the first 100\n",
    "characters of this file for illustration purposes. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0d48c89-d422-444b-b80a-6436622c39c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9afc53-e126-46f5-b7c7-cb70c93dd5ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using Python's regular expression library we will split the text to obtain a list of tokens.\n",
    "\n",
    "We are considering each individual word as a token. </div>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "For this example, we are ignoring whitespaces. For certain application you will need to keep them.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d93032f-fea4-4c39-a449-20a7c8349f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee314b39-6abd-4ca7-af8b-9a779827e602",
   "metadata": {},
   "source": [
    "## Step 2. Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461195f-e910-4d39-a20a-4815466facb7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now create a list of all unique tokens and sort them alphabetically to determine the vocabulary size: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f61577-ee88-4952-bb89-75257de7baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Size of our vocabulary:\",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5bfe4-df30-4aab-951a-3d6ac7cdb0b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a dictionary for our vocabulary and print the first 15 entries.\" </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26b7e9c7-afc7-4d7d-9603-4aeb96b9f69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:id for id, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 14:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb813b-3981-4895-a724-ff650fdc9b3c",
   "metadata": {},
   "source": [
    "## Step 3. Creating a Tokenizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe34df1-fd4d-4105-9814-aa2556866c16",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits\n",
    "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
    "vocabulary. \n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse\n",
    "integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74343c-80ee-4f2f-8f1d-03f6e107e11d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b75bedde-edc0-474e-bc72-b912ad5ad488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.encoder = vocab\n",
    "        self.decoder = {id:token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.encoder[item] for item in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.decoder[id] for id in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af954c-4f4b-4f6a-a0a2-a3e80d08d172",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a\n",
    "passage from Edith Wharton's short story to try it out in practice:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2155543-78cc-49c0-92f3-e86179bbd3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 74, 5, 241, 58, 0, 1, 53, 851, 7, 56, 1077, 115, 899, 722, 115, 361, 6, 156, 726, 1015, 361, 5, 923, 568, 988, 815, 1044, 115, 1072, 7, 1, 23, 58, 6, 115, 89, 0, 1, 53, 300, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizerv1 = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"Oh, by Jove!\" I said.\n",
    "\n",
    "It was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.\n",
    "\n",
    "\"By Jove--a Stroud!\" I cried.\"\"\"\n",
    "ids = tokenizerv1.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1abf711-a2ac-4e6d-8dca-5c91efe099e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids =  tokenizerv1.encode(\"He: how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d745e3e-e554-4d36-ba55-ba75855ac303",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Next, let's see if we can turn these token IDs back into text using the decode method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2aa6754-ccd7-4cef-9160-ebed5544bf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Oh, by Jove!\" I said. It was a sketch of a donkey -- an old tired donkey, standing in the rain under a wall.\" By Jove -- a Stroud!\" I cried.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizerv1.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab851aa-ed7a-4d33-be34-870abcf7d49d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's see what happens if we try the same with a text not incluided in our sample text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af724782-ddc4-42d7-b358-8b2757947b9e",
   "metadata": {},
   "source": [
    "text = \"Hello, how are you?\"\n",
    "ids = tokenizerv1.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457baea1-dd55-46fe-b91f-1956b8395543",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The problem is that the word \"Hello\" was not used in the The Verdict short story. \n",
    "\n",
    "Hence, it\n",
    "is not contained in the vocabulary. \n",
    "\n",
    "This highlights the need to consider large and diverse\n",
    "training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383041ff-8e65-4559-af0a-4ae63484edd4",
   "metadata": {},
   "source": [
    "## Step 4. Adding Special Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64055497-9e3e-4db2-9c46-65598ec99175",
   "metadata": {},
   "source": [
    "In the previous section, we implemented a simple tokenizer and applied it to a passage from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown words.\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and <|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc18f5-9557-4bdc-bbc2-c47886927a17",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd6378a0-a974-4348-aa93-1947098e5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a0c8ac2-7587-4159-8b81-0d62f9f77e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
