{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836193f1-3775-4bcd-a4a2-4686480a7ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38d81864-d8b3-464a-a0e0-8f27890e7688",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70388cf4-e3ab-4f03-aabb-7bbfda9e8967",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Here you can find the contents of [The Verdict](https://)\n",
    "\n",
    "Lets read and print the total number of characters followed by the first 100\n",
    "characters of this file for illustration purposes. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0d48c89-d422-444b-b80a-6436622c39c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9afc53-e126-46f5-b7c7-cb70c93dd5ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using Python's regular expression library we will split the text to obtain a list of tokens.\n",
    "\n",
    "We are considering each individual word as a token. </div>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "For this example, we are ignoring whitespaces. For certain application you will need to keep them.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d93032f-fea4-4c39-a449-20a7c8349f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee314b39-6abd-4ca7-af8b-9a779827e602",
   "metadata": {},
   "source": [
    "## Step 2. Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461195f-e910-4d39-a20a-4815466facb7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now create a list of all unique tokens and sort them alphabetically to determine the vocabulary size: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6f61577-ee88-4952-bb89-75257de7baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Size of our vocabulary:\",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5bfe4-df30-4aab-951a-3d6ac7cdb0b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a dictionary for our vocabulary and print the first 15 entries.\" </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26b7e9c7-afc7-4d7d-9603-4aeb96b9f69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:id for id, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 14:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb813b-3981-4895-a724-ff650fdc9b3c",
   "metadata": {},
   "source": [
    "## Step 3. Creating a Tokenizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe34df1-fd4d-4105-9814-aa2556866c16",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits\n",
    "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
    "vocabulary. \n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse\n",
    "integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74343c-80ee-4f2f-8f1d-03f6e107e11d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b75bedde-edc0-474e-bc72-b912ad5ad488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.encoder = vocab\n",
    "        self.decoder = {id:token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.encoder[item] for item in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.decoder[id] for id in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af954c-4f4b-4f6a-a0a2-a3e80d08d172",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a\n",
    "passage from Edith Wharton's short story to try it out in practice:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2155543-78cc-49c0-92f3-e86179bbd3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 74, 5, 241, 58, 0, 1, 53, 851, 7, 56, 1077, 115, 899, 722, 115, 361, 6, 156, 726, 1015, 361, 5, 923, 568, 988, 815, 1044, 115, 1072, 7, 1, 23, 58, 6, 115, 89, 0, 1, 53, 300, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizerv1 = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"Oh, by Jove!\" I said.\n",
    "\n",
    "It was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.\n",
    "\n",
    "\"By Jove--a Stroud!\" I cried.\"\"\"\n",
    "ids = tokenizerv1.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d745e3e-e554-4d36-ba55-ba75855ac303",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Next, let's see if we can turn these token IDs back into text using the decode method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2aa6754-ccd7-4cef-9160-ebed5544bf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Oh, by Jove!\" I said. It was a sketch of a donkey -- an old tired donkey, standing in the rain under a wall.\" By Jove -- a Stroud!\" I cried.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizerv1.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab851aa-ed7a-4d33-be34-870abcf7d49d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's see what happens if we try the same with a text not incluided in our sample text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a8fb1c9-722f-4351-8032-e8507770c80e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizerv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n",
      "Cell \u001b[1;32mIn[61], line 11\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder[item] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[1;32mIn[61], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "ids = tokenizerv1.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457baea1-dd55-46fe-b91f-1956b8395543",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The problem is that the word \"Hello\" was not used in the The Verdict short story. \n",
    "\n",
    "Hence, it\n",
    "is not contained in the vocabulary. \n",
    "\n",
    "This highlights the need to consider large and diverse\n",
    "training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383041ff-8e65-4559-af0a-4ae63484edd4",
   "metadata": {},
   "source": [
    "## Step 4. Adding Special Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64055497-9e3e-4db2-9c46-65598ec99175",
   "metadata": {},
   "source": [
    "In the previous section, we implemented a simple tokenizer and applied it to a passage from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown words.\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and <|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc18f5-9557-4bdc-bbc2-c47886927a17",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd6378a0-a974-4348-aa93-1947098e5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a0c8ac2-7587-4159-8b81-0d62f9f77e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8e6bf-b886-4f02-a418-cd9947e79599",
   "metadata": {},
   "source": [
    "## Step 5. Creating a Tokenizer Class with Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b014f49-1087-4646-88af-dfb5d1405f86",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "A simple text tokenizer that handles unknown words</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cefe55-1b63-46ae-a407-8e8186162149",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Step 1: Replace unknown words by <|unk|> tokens\n",
    "    \n",
    "Step 2: Replace spaces before the specified punctuations\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7dbb4843-bbe3-40e7-8528-582932e3e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.encoder = vocab\n",
    "        self.decoder = {id:token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.;:?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [\n",
    "            item if item in self.encoder\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.encoder[token] for token in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.decoder[id] for id in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.;:?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "516b0a6e-7648-4502-ba1a-0c00832eabe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 560, 169, 1126, 10]\n",
      "<|unk|>, how are you?\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "tokenizerv2 = SimpleTokenizerV2(vocab)\n",
    "ids = tokenizerv2.encode(text)\n",
    "print(ids)\n",
    "print(tokenizerv2.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6dcc2-7669-4680-a788-cfc2a6d7463c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Based on the detokenized output, we can know that \"Hello\" is not part of the vocabulary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bedc5c7-68bb-4ea0-a964-51476f397149",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Depending on the LLM, some researchers also consider additional special tokens suchas the following.\n",
    "\n",
    "* [BOS] (beginning of sequence): This token marks the start of a text.\n",
    "\n",
    "* [EOS] (end of sequence): This token is positioned at the end of a text, and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>.\n",
    "\n",
    "* [PAD] (padding): To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10db6ae-b2f4-46ed-9b61-451709f565e9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned above but only uses an <|endoftext|> token for simplicity. Instead, GPT models use a byte pair encoding tokenizer, which breaks down words into subword units\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c3163-771f-4aab-ac6a-c0c22eea9a64",
   "metadata": {},
   "source": [
    "## Step 6. Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85a33ab-3801-4023-b72c-fa13183b4122",
   "metadata": {},
   "source": [
    "**BPE Tokenizer**\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Since implementing BPE can be relatively complicated, we will use an existing Python\n",
    "open-source library called tiktoken (https://github.com/openai/tiktoken). \n",
    "\n",
    "This library implements\n",
    "the BPE algorithm very efficiently based on source code in Rust.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42e467-6bf4-4f69-b63b-33dd15c42e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5743b2c-c34f-4540-86cc-137167714314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e9ab9-e52d-442b-8e7f-b8e5c70cc2ae",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eadfe3fd-8f8e-4174-8f8f-27b7bb7f4cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a84ef5-2c22-418b-bd1d-a86bce63313a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via\n",
    "an encode method:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7401e6b2-aeb4-4e09-88dd-7b4b979d8bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1939a-f8f4-4b53-880b-39520c7c41da",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We can then convert the token IDs back into text using the decode method, similar to our\n",
    "SimpleTokenizerV2 earlier:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc5ac15c-1a2c-45ee-bcbb-9022b9ee12e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e1c31-f94a-4166-a693-73489039f534",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The BPE Tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of **50,527**, with <|endoftext|> being assigned the largest token ID.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de23a90-a064-42c7-a602-a5a247c96c6a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary (e.g. someunknownPlace) into smaller subword units or even individual characters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35300034-480e-4e74-adec-b39e3b4e2834",
   "metadata": {},
   "source": [
    "**Let us take another simple example to illustrate how the BPE tokenizer deals with unknown tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9e1caa62-b0b3-497f-b187-46d895e7437a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2cdc9-0253-49be-9069-0cf907a1cd53",
   "metadata": {},
   "source": [
    "## Step 7. Creating Input-Target pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8989a-7998-4327-aaea-ec37ead89dcf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156569a0-4843-4f58-b48e-90922ff32bb7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To get started, we will first tokenize the whole The Verdict short story we worked with\n",
    "earlier using the BPE tokenizer introduced in the previous section:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "87f4c34d-280c-48c4-bf11-961ba46b7a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be93801-ad53-4fc9-a5a2-646b81954655",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Executing the code above will return 5145, the total number of tokens in the training set, after applying the BPE tokenizer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bf8cb-9d4c-4135-b766-3c86e5cdb126",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1 (The context size determines how many tokens are included in the input):</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a40721f0-deed-491a-9064-f88997b6a953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [40, 367, 2885, 1464]\n",
      "y:     [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 # length of the input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) \n",
    "#to predict the next word in the sequence. \n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38008c16-d1da-4a23-9d0f-cc17e1fa1f29",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c3861cd5-dea2-4d3b-8891-d27618cfb241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] ---> 367\n",
      "[40, 367] ---> 2885\n",
      "[40, 367, 2885] ---> 1464\n",
      "[40, 367, 2885, 1464] ---> 1807\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_size):\n",
    "    context = enc_text[:i+1]\n",
    "    desired = enc_text[i+1]\n",
    "    print(context, \"--->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521dacfc-82e7-406d-8096-4f0398e1dc8c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token ID on the right side of the arrow represents the target token ID  that the LLM is supposed to predict.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32359be7-2cc9-418f-8896-fdcba6637299",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For illustration purposes, let's repeat the previous code but convert the token IDs into\n",
    "text:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b3ddec1d-151e-400d-b76c-51b2a8fa2197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --->  H\n",
      "I H ---> AD\n",
      "I HAD --->  always\n",
      "I HAD always --->  thought\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_size):\n",
    "    context = enc_text[:i+1]\n",
    "    desired = enc_text[i+1]\n",
    "    print(tokenizer.decode(context), \"--->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bfa9f-b4e3-4172-8dc8-89c93e5bda47",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138169b7-996b-454d-b058-01cf64a59d5d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "In particular, we are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict,\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c35a7-660b-418a-b4fd-a720f5d3a815",
   "metadata": {},
   "source": [
    "## Step 8. Implementing a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800a43d-cb8b-49a1-aa68-b528f19620cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and DataLoader classes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ee76d4-f78a-4d26-a925-5afdd13404b7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Step 1:** Tokenize the entire text\n",
    "    \n",
    "**Step 2:** Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "\n",
    "**Step 3:** Return the total number of rows in the dataset\n",
    "\n",
    "**Step 4:** Return a single row from the dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ae3dbfb3-d7da-418b-8018-0a529fc35ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "        # Use a sliding window chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf7aa7e-e6ee-436c-beab-b53ae9dce19e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The following code will use the GPTDatasetV1 to load the inputs in batches via a PyTorch DataLoader:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a33739-7096-4523-8c46-ca57eacf9fca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Step 1**: Initialize the tokenizer\n",
    "\n",
    "**Step 2:** Create dataset\n",
    "\n",
    "**Step 3:** drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
    "\n",
    "**Step 4:** The number of CPU processes to use for preprocessing\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3706f21d-06a8-46c9-b923-23bc1a5aad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dacbea-2908-48d3-b774-bf720d947cc0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4, \n",
    "\n",
    "This will develop an intuition of how the GPTDatasetV1 class and the `create_dataloader_v1` function work together: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fd40afa7-9295-4c46-8d84-458aa58b80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c486c0c-c234-44a3-a356-ef583eea1ddd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3a42e9cc-a6ed-4319-9474-53a875a4ee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.7.1+cpu\n",
      "first_batch: [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pytorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(\"first_batch:\", first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c0079-b235-4fb0-a869-cfa8cb04394b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The first_batch variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs.\n",
    "\n",
    "Since the max_length is set to 4, each of the two tensors contains 4 token IDs. \n",
    "\n",
    "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least\n",
    "256.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9255729-86aa-45d3-9ee3-d8d5449bfda7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "To illustrate the meaning of stride=1, let's fetch another batch from this dataset: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3fc5b2bf-acd1-450b-9392-2db60a73e424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_batch: [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(\"second_batch:\", second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebbc4c-2176-4f6f-ab78-d71e7ccf0469",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "If we compare the first with the second batch, we can see that the second batch's token IDs are shifted by one position compared to the first batch. \n",
    "\n",
    "For example, the second ID in the first batch's input is 367, which is the first ID of the second batch's input. \n",
    "\n",
    "The stride setting dictates the number of positions the inputs shift across batches, emulating a sliding window approach\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442092f5-e1ef-4eb4-9193-d0f247a20187",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Before we move on to creating the embedding vectors from the token IDs, let's have a brief look at how we can use the data loader to sample with a batch size greater than 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3d63466-594c-47d3-98f6-6c1d3ffb53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257],\n",
      "        [10899,  2138,   257,  7026]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8,max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be652950-7fd0-40da-b852-fdd89c6f93be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Note that we increase the stride to 4. This is to utilize the data set fully (we don't skip a single word) but also avoid any overlap between the batches, since more overlap could lead to increased overfitting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81f7d4-b404-49f4-a55e-5731e210185c",
   "metadata": {},
   "source": [
    "## Step 9. Creating Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778ed1a-9463-481e-96ed-d363e3717b38",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's illustrate how the token ID to embedding vector conversion works with a hands-on example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "26678569-61a1-4b7e-b835-28ebbe2b4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3 ,5 ,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7f4af-9f7b-4b2d-81d6-9144a1f66a2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d1baf-36b1-47d4-a789-84d507af1338",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch, setting the random seed to 100 for reproducibility purposes:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a7b0e809-78b1-4b0a-8e81-68a171695186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1268,  1.3564, -0.0247],\n",
      "        [-0.8466,  0.0293, -0.5721],\n",
      "        [-1.2546,  0.0486,  0.2753],\n",
      "        [-2.1550, -0.7116,  0.0575],\n",
      "        [ 0.6263, -1.7736, -0.2205],\n",
      "        [ 2.7467, -1.0480,  1.1239]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(100)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea2ec6-3350-4d30-a62d-c4d124be958a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We can see that the weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training as part of the LLM optimization itself. Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary. And there is one column for each of the three embedding dimensions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f53c0-9c19-493b-aa68-4a47ef8624e7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the embedding vector:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2faf9fe5-936d-4f19-afb1-951c810244df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1550, -0.7116,  0.0575]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7651b1-980d-4ee1-ac46-518468e54809",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the 4th row. In other words, the embedding layer is essentially a look-up operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e8af3-d0ce-47d0-9bda-30838d0435dd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now apply that to all four input IDs we defined earlier (torch.tensor([2, 3, 5, 1])):\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "960bc6c1-b8d4-4db9-81f4-f693baf1ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2546,  0.0486,  0.2753],\n",
      "        [-2.1550, -0.7116,  0.0575],\n",
      "        [ 2.7467, -1.0480,  1.1239],\n",
      "        [-0.8466,  0.0293, -0.5721]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8d95f-9bdf-4660-b090-770c28f289ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9849e9-2804-454e-9639-288052bd0f89",
   "metadata": {},
   "source": [
    "## Step 10. Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c11b8-78a8-40e1-8dc5-6786a6f76343",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now consider more realistic embedding sizes and encode the input tokens into a 256-dimensional vector representation. \n",
    "\n",
    "This is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable for experimentation. \n",
    "\n",
    "Furthermore, we assume that the token IDs were created by the BPE tokenizer that we implemented earlier, which has a vocabulary size of 50,257:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d8893f5f-d141-4cd4-968b-37ef0f469892",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787a36f-7f29-4877-84ac-7f06ceb21f32",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Using the token_embedding_layer above, if we sample data from the data loader, we embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21c6e0-65ee-4ef9-935a-18c3c32f4b3e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's instantiate the data loader ( Data sampling with a sliding window), first:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "440da1f9-ee7f-42b6-87bd-75b6837461d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "faca8f4e-3deb-4669-b94b-7f513163824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs: \\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073cae3b-ff38-4f04-b623-f8d50e49e4b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data batch consists of 8 text samples with 4 tokens each.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
