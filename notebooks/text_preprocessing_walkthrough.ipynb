{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836193f1-3775-4bcd-a4a2-4686480a7ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38d81864-d8b3-464a-a0e0-8f27890e7688",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70388cf4-e3ab-4f03-aabb-7bbfda9e8967",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Here you can find the contents of [The Verdict](https://)\n",
    "\n",
    "Lets read and print the total number of characters followed by the first 100\n",
    "characters of this file for illustration purposes. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0d48c89-d422-444b-b80a-6436622c39c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9afc53-e126-46f5-b7c7-cb70c93dd5ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using Python's regular expression library we will split the text to obtain a list of tokens.\n",
    "\n",
    "We are considering each individual word as a token. </div>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "For this example, we are ignoring whitespaces. For certain application you will need to keep them.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d93032f-fea4-4c39-a449-20a7c8349f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee314b39-6abd-4ca7-af8b-9a779827e602",
   "metadata": {},
   "source": [
    "## Step 2. Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461195f-e910-4d39-a20a-4815466facb7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now create a list of all unique tokens and sort them alphabetically to determine the vocabulary size: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6f61577-ee88-4952-bb89-75257de7baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Size of our vocabulary:\",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5bfe4-df30-4aab-951a-3d6ac7cdb0b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a dictionary for our vocabulary and print the first 15 entries.\" </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26b7e9c7-afc7-4d7d-9603-4aeb96b9f69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:id for id, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 14:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb813b-3981-4895-a724-ff650fdc9b3c",
   "metadata": {},
   "source": [
    "## Step 3. Creating a Tokenizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe34df1-fd4d-4105-9814-aa2556866c16",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits\n",
    "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
    "vocabulary. \n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse\n",
    "integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74343c-80ee-4f2f-8f1d-03f6e107e11d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b75bedde-edc0-474e-bc72-b912ad5ad488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.encoder = vocab\n",
    "        self.decoder = {id:token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.encoder[item] for item in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.decoder[id] for id in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af954c-4f4b-4f6a-a0a2-a3e80d08d172",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a\n",
    "passage from Edith Wharton's short story to try it out in practice:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2155543-78cc-49c0-92f3-e86179bbd3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 74, 5, 241, 58, 0, 1, 53, 851, 7, 56, 1077, 115, 899, 722, 115, 361, 6, 156, 726, 1015, 361, 5, 923, 568, 988, 815, 1044, 115, 1072, 7, 1, 23, 58, 6, 115, 89, 0, 1, 53, 300, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizerv1 = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"Oh, by Jove!\" I said.\n",
    "\n",
    "It was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.\n",
    "\n",
    "\"By Jove--a Stroud!\" I cried.\"\"\"\n",
    "ids = tokenizerv1.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d745e3e-e554-4d36-ba55-ba75855ac303",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Next, let's see if we can turn these token IDs back into text using the decode method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2aa6754-ccd7-4cef-9160-ebed5544bf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Oh, by Jove!\" I said. It was a sketch of a donkey -- an old tired donkey, standing in the rain under a wall.\" By Jove -- a Stroud!\" I cried.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizerv1.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab851aa-ed7a-4d33-be34-870abcf7d49d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's see what happens if we try the same with a text not incluided in our sample text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a8fb1c9-722f-4351-8032-e8507770c80e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizerv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n",
      "Cell \u001b[1;32mIn[61], line 11\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder[item] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[1;32mIn[61], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "ids = tokenizerv1.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457baea1-dd55-46fe-b91f-1956b8395543",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The problem is that the word \"Hello\" was not used in the The Verdict short story. \n",
    "\n",
    "Hence, it\n",
    "is not contained in the vocabulary. \n",
    "\n",
    "This highlights the need to consider large and diverse\n",
    "training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383041ff-8e65-4559-af0a-4ae63484edd4",
   "metadata": {},
   "source": [
    "## Step 4. Adding Special Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64055497-9e3e-4db2-9c46-65598ec99175",
   "metadata": {},
   "source": [
    "In the previous section, we implemented a simple tokenizer and applied it to a passage from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown words.\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and <|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc18f5-9557-4bdc-bbc2-c47886927a17",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd6378a0-a974-4348-aa93-1947098e5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a0c8ac2-7587-4159-8b81-0d62f9f77e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8e6bf-b886-4f02-a418-cd9947e79599",
   "metadata": {},
   "source": [
    "## Step 5. Creating a Tokenizer Class with Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b014f49-1087-4646-88af-dfb5d1405f86",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "A simple text tokenizer that handles unknown words</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cefe55-1b63-46ae-a407-8e8186162149",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Step 1: Replace unknown words by <|unk|> tokens\n",
    "    \n",
    "Step 2: Replace spaces before the specified punctuations\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7dbb4843-bbe3-40e7-8528-582932e3e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.encoder = vocab\n",
    "        self.decoder = {id:token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.;:?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [\n",
    "            item if item in self.encoder\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.encoder[token] for token in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.decoder[id] for id in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.;:?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "516b0a6e-7648-4502-ba1a-0c00832eabe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 560, 169, 1126, 10]\n",
      "<|unk|>, how are you?\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "tokenizerv2 = SimpleTokenizerV2(vocab)\n",
    "ids = tokenizerv2.encode(text)\n",
    "print(ids)\n",
    "print(tokenizerv2.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6dcc2-7669-4680-a788-cfc2a6d7463c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Based on the detokenized output, we can know that \"Hello\" is not part of the vocabulary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bedc5c7-68bb-4ea0-a964-51476f397149",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Depending on the LLM, some researchers also consider additional special tokens suchas the following.\n",
    "\n",
    "* [BOS] (beginning of sequence): This token marks the start of a text.\n",
    "\n",
    "* [EOS] (end of sequence): This token is positioned at the end of a text, and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>.\n",
    "\n",
    "* [PAD] (padding): To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10db6ae-b2f4-46ed-9b61-451709f565e9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned above but only uses an <|endoftext|> token for simplicity. Instead, GPT models use a byte pair encoding tokenizer, which breaks down words into subword units\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
